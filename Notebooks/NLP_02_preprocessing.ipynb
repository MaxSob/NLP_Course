{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP Syntactic parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Syntactic parsing is a task in NLP which verifies the structure of a given text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lowercasing\n",
    "One of the simplest methods to standarize the input and normalizer the capitalization is to convert the input in it's lower case form"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['canada', 'canada', 'canada', 'canada']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts=[\"CANADA\",\"Canada\",\"canadA\",\"canada\"]\n",
    "lower_words=[word.lower() for word in texts]\n",
    "lower_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming\n",
    "Stemming use a method to truncate similar words in order to give them a common base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**\n",
    "Make a function which recieves a list of root words and a string and replace similar strings with it's root according to a treshold\n",
    "\n",
    "Hint: check python implementations of the levenshtein function https://pypi.org/project/python-Levenshtein/, and install a library to check the distance between two strings https://github.com/SoldAI/hermetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: hermetrics in /home/mcampos/anaconda3/lib/python3.7/site-packages (0.1.13)\n"
     ]
    }
   ],
   "source": [
    "!pip install hermetrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "0.4\n",
      "0.6\n"
     ]
    }
   ],
   "source": [
    "from hermetrics.levenshtein import Levenshtein\n",
    "\n",
    "lev = Levenshtein()\n",
    "\n",
    "print(lev.distance('ace', 'abcde'))\n",
    "print(lev.normalized_distance('ace', 'abcde'))\n",
    "print(lev.similarity('ace', 'abcde'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['connect', 'connect', 'connect', 'trouble']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from Levenshtein import ratio\n",
    "\n",
    "def stemming(roots, string, threshold = 0.6):\n",
    "    words = []\n",
    "    #Your code here\n",
    "    word_list = string.lower().split(\" \")\n",
    "    for w in word_list:\n",
    "        for r in roots:\n",
    "            if ratio(r, w) > threshold:\n",
    "                words.append(r)\n",
    "                break\n",
    "    print(words)\n",
    "    return words\n",
    "\n",
    "def testStemming():\n",
    "    test = (['connect', \"trouble\"], \n",
    "              [\"connect\", \"connect\", \"connect\", \"trouble\", \"not\", \"to\", \"replace\"])\n",
    "    if stemming(test[0], \"Connecting, connected, connection is troubled not to replace\") != test[1]:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "testStemming()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# init stemmer\n",
    "porter_stemmer=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_word</th>\n",
       "      <th>stemmed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>connect</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>connected</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>connection</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>connections</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>connects</td>\n",
       "      <td>connect</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  original_word stemmed_word\n",
       "0       connect      connect\n",
       "1     connected      connect\n",
       "2    connection      connect\n",
       "3   connections      connect\n",
       "4      connects      connect"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem connect variations\n",
    "words=[\"connect\",\"connected\",\"connection\",\"connections\",\"connects\"]\n",
    "stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "\n",
    "stemdf= pd.DataFrame({'original_word': words,'stemmed_word': stemmed_words})\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_word</th>\n",
       "      <th>stemmed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trouble</td>\n",
       "      <td>troubl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>troubled</td>\n",
       "      <td>troubl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>troubles</td>\n",
       "      <td>troubl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>troublemsome</td>\n",
       "      <td>troublemsom</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  original_word stemmed_word\n",
       "0       trouble       troubl\n",
       "1      troubled       troubl\n",
       "2      troubles       troubl\n",
       "3  troublemsome  troublemsom"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem trouble variations\n",
    "words=[\"trouble\",\"troubled\",\"troubles\",\"troublemsome\"]\n",
    "stemmed_words=[porter_stemmer.stem(word=word) for word in words]\n",
    "\n",
    "stemdf= pd.DataFrame({'original_word': words,'stemmed_word': stemmed_words})\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/mcampos/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# init lemmatizer\n",
    "lemmatizer = WordNetLemmatizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_word</th>\n",
       "      <th>lemmatized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trouble</td>\n",
       "      <td>trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>troubling</td>\n",
       "      <td>trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>troubled</td>\n",
       "      <td>trouble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>troubles</td>\n",
       "      <td>trouble</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  original_word lemmatized_word\n",
       "0       trouble         trouble\n",
       "1     troubling         trouble\n",
       "2      troubled         trouble\n",
       "3      troubles         trouble"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatize trouble variations\n",
    "words=[\"trouble\",\"troubling\",\"troubled\",\"troubles\",]\n",
    "lemmatized_words=[lemmatizer.lemmatize(word=word,pos='v') for word in words]\n",
    "lemmatizeddf= pd.DataFrame({'original_word': words,'lemmatized_word': lemmatized_words})\n",
    "lemmatizeddf=lemmatizeddf[['original_word','lemmatized_word']]\n",
    "lemmatizeddf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>original_word</th>\n",
       "      <th>lemmatized_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>goose</td>\n",
       "      <td>goose</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>geese</td>\n",
       "      <td>goose</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  original_word lemmatized_word\n",
       "0         goose           goose\n",
       "1         geese           goose"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lemmatize goose variations\n",
    "words=[\"goose\",\"geese\"]\n",
    "lemmatized_words=[lemmatizer.lemmatize(word=word,pos='n') for word in words]\n",
    "lemmatizeddf= pd.DataFrame({'original_word': words,'lemmatized_word': lemmatized_words})\n",
    "lemmatizeddf=lemmatizeddf[['original_word','lemmatized_word']]\n",
    "lemmatizeddf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stop Word Removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords=['this','that','and','a','we','it','to','is','of','up','need']\n",
    "text=\"this is a text full of content and we need to clean it up\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original sentence =  this is a text full of content and we need to clean it up\n",
      "sentence with stop words removed=  W W W text full W content W W W W clean W W\n"
     ]
    }
   ],
   "source": [
    "words=text.split(\" \")\n",
    "shortlisted_words=[]\n",
    "\n",
    "#remove stop words\n",
    "for w in words:\n",
    "    if w not in stopwords:\n",
    "        shortlisted_words.append(w)\n",
    "    else:\n",
    "        shortlisted_words.append(\"W\")\n",
    "\n",
    "print(\"original sentence = \",text)    \n",
    "print(\"sentence with stop words removed= \",' '.join(shortlisted_words))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Noise Removal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**:\n",
    "Make a function to clean a string and to adapt the case of the words to lowercase, and to remove special characters. The function cleanString must recieve a string of the list of characters to eliminate and the string to clean, and shall return the lowercase clean string.\n",
    "\n",
    "Hint: you can use trim, split, and replace string functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed at la gata '' maulla @ en la noche\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def cleanString(special_chars, string):\n",
    "    clean_string = string.lower()\n",
    "    #Your code here\n",
    "    return clean_string\n",
    "    \n",
    "\n",
    "def test():\n",
    "    special_chars = \",.@?!¬-\\''=()\"\n",
    "    tests = [\n",
    "        \"La gata '' maulla @ en la noche\",\n",
    "        \"       ToDo va bien    aquí\"\n",
    "    ]\n",
    "    targets = [\n",
    "        \"la gata maulla en la noche\",\n",
    "        \"todo va bien aquí\"\n",
    "    ]\n",
    "    for i in range(len(tests)):\n",
    "        if cleanString(special_chars, tests[i]) != targets[i]:\n",
    "            print(\"Failed at \" + cleanString(special_chars, tests[i]))\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import pandas as pd\n",
    "import re\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "porter_stemmer=PorterStemmer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_word</th>\n",
       "      <th>stemmed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..trouble..</td>\n",
       "      <td>..trouble..</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble&lt;</td>\n",
       "      <td>trouble&lt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trouble!</td>\n",
       "      <td>trouble!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;a&gt;trouble&lt;/a&gt;</td>\n",
       "      <td>&lt;a&gt;trouble&lt;/a&gt;</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.trouble</td>\n",
       "      <td>1.troubl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         raw_word    stemmed_word\n",
       "0     ..trouble..     ..trouble..\n",
       "1        trouble<        trouble<\n",
       "2        trouble!        trouble!\n",
       "3  <a>trouble</a>  <a>trouble</a>\n",
       "4       1.trouble        1.troubl"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem raw words with noise\n",
    "raw_words=[\"..trouble..\",\"trouble<\",\"trouble!\",\"<a>trouble</a>\",'1.trouble']\n",
    "stemmed_words=[porter_stemmer.stem(word=word) for word in raw_words]\n",
    "stemdf= pd.DataFrame({'raw_word': raw_words,'stemmed_word': stemmed_words})\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regular expressions\n",
    "Regular expressions are useful to find patterns in text, using a special annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+529994263828\n",
      "mario.campos@upy.edu.mx\n",
      "mario.campos.soberanis@gmail.com\n",
      "com y trabajo en soldai sapi de C.V.\n",
      "Soy mario mi numero es a phone y mi email es mario.campos@upy.edu.mx y mario.campos.soberanis@gmail.com y trabajo en soldai sapi de C.V.\n"
     ]
    }
   ],
   "source": [
    "def match_patterns(pattern, string):\n",
    "    match = re.findall(pattern, string)\n",
    "    for match in re.finditer(pattern, string):\n",
    "        print(match.group())\n",
    "    \n",
    "phone_pattern = \"\\+[0-9]{12}\"\n",
    "email_pattern = \"((\\w)*\\.?)+@((\\w)*\\.?)+\\.(mx|com)\"\n",
    "company_pattern = \"([\\w\\s]*[cC]\\.[vV]\\.)\"\n",
    "\n",
    "string_search = \"Soy mario mi numero es +529994263828 y mi email es mario.campos@upy.edu.mx y mario.campos.soberanis@gmail.com y trabajo en soldai sapi de C.V.\"\n",
    "\n",
    "match_patterns(phone_pattern, string_search)\n",
    "match_patterns(email_pattern, string_search)\n",
    "match_patterns(company_pattern, string_search)\n",
    "\n",
    "text = re.sub(phone_pattern, \"a phone\" ,string_search)\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_words(text):\n",
    "    \"\"\"Basic cleaning of texts.\"\"\"\n",
    "    \n",
    "    # remove html markup\n",
    "    text=re.sub(\"(<.*?>)\",\"\",text)\n",
    "    \n",
    "    #remove non-ascii and digits\n",
    "    text=re.sub(\"(\\\\W|\\\\d)\",\" \",text)\n",
    "    \n",
    "    #remove whitespace\n",
    "    text=text.strip()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>raw_word</th>\n",
       "      <th>cleaned_word</th>\n",
       "      <th>stemmed_word</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>..trouble..</td>\n",
       "      <td>trouble</td>\n",
       "      <td>troubl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>trouble&lt;</td>\n",
       "      <td>trouble</td>\n",
       "      <td>troubl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>trouble!</td>\n",
       "      <td>trouble</td>\n",
       "      <td>troubl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>&lt;a&gt;trouble&lt;/a&gt;</td>\n",
       "      <td>trouble</td>\n",
       "      <td>troubl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1.trouble</td>\n",
       "      <td>trouble</td>\n",
       "      <td>troubl</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         raw_word cleaned_word stemmed_word\n",
       "0     ..trouble..      trouble       troubl\n",
       "1        trouble<      trouble       troubl\n",
       "2        trouble!      trouble       troubl\n",
       "3  <a>trouble</a>      trouble       troubl\n",
       "4       1.trouble      trouble       troubl"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# stem words already cleaned\n",
    "cleaned_words=[scrub_words(w) for w in raw_words]\n",
    "cleaned_stemmed_words=[porter_stemmer.stem(word=word) for word in cleaned_words]\n",
    "stemdf= pd.DataFrame({'raw_word': raw_words,'cleaned_word':cleaned_words,'stemmed_word': cleaned_stemmed_words})\n",
    "stemdf=stemdf[['raw_word','cleaned_word','stemmed_word']]\n",
    "stemdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Tokenizing\n",
    "In computer science, lexical analysis, lexing or tokenization is the process of converting a sequence of characters (such as in a computer program or web page) into a sequence of tokens (strings with an assigned and thus identified meaning). A program that performs lexical analysis may be termed a lexer, tokenizer, or scanner, though scanner is also a term for the first stage of a lexer. A lexer is generally combined with a parser, which together analyze the syntax of programming languages, web pages, and so forth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise**: Perform a basic tokenization method which divdes the paragraph using an arbitrary string of punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraph = \"Unlike research, where you are inventing and creating new areas of knowledge, programming is all about applying existing knowledge. Books, research papers, online articles, and learning videos become resources that you will use often. There’s no need to memorize anything. You can always reach for a resource to find the answer. Memorization comes automatically and naturally as you work on more projects\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpleTokenize(paragraph, punctuation):\n",
    "    sentences = []\n",
    "    # Your code here\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test your code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n",
      "[]\n",
      "Failed at: Unlike research, where you are inventing and creating new areas of knowledge, programming is all about applying existing knowledge. Books, research papers, online articles, and learning videos become resources that you will use often. There’s no need to memorize anything. You can always reach for a resource to find the answer. Memorization comes automatically and naturally as you work on more projects with punctuation .\n",
      "Expected: ['Unlike research, where you are inventing and creating new areas of knowledge, programming is all about applying existing knowledge', ' Books, research papers, online articles, and learning videos become resources that you will use often', ' There’s no need to memorize anything', ' You can always reach for a resource to find the answer', ' Memorization comes automatically and naturally as you work on more projects']\n",
      "Got: []\n",
      "False\n",
      "Failed at: Unlike research, where you are inventing and creating new areas of knowledge, programming is all about applying existing knowledge. Books, research papers, online articles, and learning videos become resources that you will use often. There’s no need to memorize anything. You can always reach for a resource to find the answer. Memorization comes automatically and naturally as you work on more projects with punctuation .,\n",
      "Expected: ['Unlike research', ' where you are inventing and creating new areas of knowledge', ' programming is all about applying existing knowledge', ' Books', ' research papers', ' online articles', ' and learning videos become resources that you will use often', ' There’s no need to memorize anything', ' You can always reach for a resource to find the answer', ' Memorization comes automatically and naturally as you work on more projects']\n",
      "Got: []\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "tests_1 = (paragraph, \".\", ['Unlike research, where you are inventing and creating new areas of knowledge, programming is all about applying existing knowledge', ' Books, research papers, online articles, and learning videos become resources that you will use often', ' There’s no need to memorize anything', ' You can always reach for a resource to find the answer', ' Memorization comes automatically and naturally as you work on more projects'])\n",
    "tests_2 = (paragraph, \".,\", ['Unlike research', ' where you are inventing and creating new areas of knowledge', ' programming is all about applying existing knowledge', ' Books', ' research papers', ' online articles', ' and learning videos become resources that you will use often', ' There’s no need to memorize anything', ' You can always reach for a resource to find the answer', ' Memorization comes automatically and naturally as you work on more projects'])\n",
    "\n",
    "def testTokenizer(test):\n",
    "    result = simpleTokenize(test[0], test[1])\n",
    "    if result != test[2]:\n",
    "        print(\"Failed at: \" + str(test[0]) + \" with punctuation \" + str(test[1]))\n",
    "        print(\"Expected: \" +  str(test[2]))\n",
    "        print(\"Got: \" + str(result))\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "print(simpleTokenize(paragraph, \".\"))\n",
    "print(simpleTokenize(paragraph, \".,\"))\n",
    "print(testTokenizer(tests_1))\n",
    "print(testTokenizer(tests_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This\n",
      "is\n",
      "awesome\n",
      "guys\n",
      ":)\n"
     ]
    }
   ],
   "source": [
    "phrase = \"This is awesome guys :)\"\n",
    "tokenizer = nltk.tokenize.TweetTokenizer()\n",
    "for t in tokenizer.tokenize(phrase):\n",
    "  print(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's load and  the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Teacher',\n",
       " 'NLP_02_preprocessing.ipynb',\n",
       " 'NLP_01_intro.ipynb',\n",
       " '.ipynb_checkpoints']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>senti</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>298.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.006711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.748483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>2.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            senti\n",
       "count  298.000000\n",
       "mean     2.006711\n",
       "std      1.748483\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      2.000000\n",
       "75%      4.000000\n",
       "max      4.000000"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir_data = \"../data/\"\n",
    "df = pd.read_csv(dir_data + 'finalizedtest.csv')\n",
    "df.describe()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
